= Assigning Pods to Nodes

== Introduction

This scenario illustrates how to deploy applications using Runtime Component Operator and assigning application pods to specific nodes in a Kubernetes cluster.

You will deploy instances of two applications, `coffeeshop-frontend` and `coffeeshop-backend`, co-located on the same nodes with SSD storage type.

This scenario is inspired by examples from link:++https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node++[Assigning Pods to Nodes] Kubernetes tutorial.

== Affinity

For this example, imagine a multi-node cluster has a mix of nodes including nodes that use SSD storage. The nodes that use SSD storage are labelled with `disktype: ssd`. You can use the following command to label nodes:

[source]
----
oc label nodes <node-name> <label-key>=<label-value>
----

You can verify labels specified on nodes by re-running the following command and checking that the node now has a label:

[source]
----
oc get nodes --show-labels
----

The output the command would look like the following output. As you can see, the only nodes that are labeled with `disktype=ssd` are `worker0` and `worker1` nodes.

[source]
----
oc get nodes --show-labels
NAME                             STATUS   ROLES    AGE   VERSION   LABELS
master0.shames.os.fyre.ibm.com   Ready    master   26d   v1.16.2   beta.kubernetes.io/arch=amd64,...
master1.shames.os.fyre.ibm.com   Ready    master   26d   v1.16.2   beta.kubernetes.io/arch=amd64,...
master2.shames.os.fyre.ibm.com   Ready    master   26d   v1.16.2   beta.kubernetes.io/arch=amd64,...
worker0.shames.os.fyre.ibm.com   Ready    worker   26d   v1.16.2   disktype=ssd,beta.kubernetes.io/arch=amd64...
worker1.shames.os.fyre.ibm.com   Ready    worker   26d   v1.16.2   disktype=ssd,beta.kubernetes.io/arch=amd64...
worker2.shames.os.fyre.ibm.com   Ready    worker   26d   v1.16.2   beta.kubernetes.io/arch=amd64,...
----

In this cluster, we want the `coffeeshop-frontend` and `coffeeshop-backend` applications to be co-located on the same nodes with SSD storage types.

Here is a YAML snippet of the `RuntimeComponent` custom resource (CR) for the `coffeeshop-backend` application with two replicas. The custom resource has `.spec.affinity.podAntiAffinity` configured to ensure the scheduler does not co-locate pods for the application on a single node. As described in the link:++https://github.com/application-stacks/runtime-component-operator/blob/master/doc/user-guide.adoc#labels++[user guide], the application pods are labelled with `app.kubernetes.io/instance: metadata.name`.

The custom resource also has `spec.affinity.nodeAffinityLabels` to ensure the application pods will run on nodes with the `disktype: ssd` label.

[source,yaml]
----
apiVersion: app.stacks/v1beta1
kind: RuntimeComponent
metadata:
  name: coffeeshop-backend
spec:
  applicationImage: 'k8s.gcr.io/pause:2.0'
  replicas: 2
  affinity:
    nodeAffinityLabels:
      disktype: ssd
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                  - coffeeshop-backend
          topologyKey: kubernetes.io/hostname
----

The YAML snippet below shows the `RuntimeComponent` CR for the `coffeeshop-frontend` application. The snippet has the `.spec.affinity.podAffinity` configured which informs the scheduler that all its replicas are to be co-located with pods that have selector label `app.kubernetes.io/instance: coffeeshop-backend`. The CR also includes `.spec.affinity.podAntiAffinity` to ensure that each `coffeeshop-frontend` replica does not co-locate on a single node. In addition, the `spec.affinity.nodeAffinityLabels` parameter is specified to ensure the application pods will run on nodes with the `disktype: ssd` label.

[source,yaml]
----
apiVersion: app.stacks/v1beta1
kind: RuntimeComponent
metadata:
  name: coffeeshop-frontend
spec:
  applicationImage: 'k8s.gcr.io/pause:2.0'
  replicas: 2
  affinity:
    nodeAffinityLabels:
      disktype: ssd
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                  - coffeeshop-frontend
          topologyKey: kubernetes.io/hostname
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                  - coffeeshop-backend
          topologyKey: kubernetes.io/hostname
----

To verify which nodes the pods are scheduled on, run the following command:

[source]
----
oc get pods -o wide
----

And the output would look like the following:

[source]
----
NAME                                   READY   STATUS    RESTARTS   AGE     IP              NODE
coffeeshop-backend-f57557d99-5lm24     1/1     Running   0          6m8s    10.254.20.173   worker0.shames.os.fyre.ibm.com
coffeeshop-backend-f57557d99-fv62n     1/1     Running   0          6m8s    10.254.5.39     worker1.shames.os.fyre.ibm.com
coffeeshop-frontend-789c585488-5k7sh   1/1     Running   0          5m52s   10.254.5.38     worker1.shames.os.fyre.ibm.com
coffeeshop-frontend-789c585488-9m529   1/1     Running   0          5m52s   10.254.20.172   worker0.shames.os.fyre.ibm.com
----

As you can see in the output, no more than one pod per application is running on a single node. Also, the pods for the two applications are co-located on a same node.

This scenario illustrated how to use the link:++https://github.com/application-stacks/runtime-component-operator/blob/master/doc/user-guide.adoc#affinity++[Affinity] feature in the Runtime Component Operator to assign pods to certain nodes within a cluster.
